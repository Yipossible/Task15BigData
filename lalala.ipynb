{"nbformat_minor":2,"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","file_extension":".py","mimetype":"text/x-python","version":"3.6.0"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"cells":[{"outputs":[{"output_type":"stream","name":"stdout","text":["3.14165616\n"]}],"metadata":{"collapsed":false},"execution_count":4,"source":["import findspark\n","findspark.init()\n","import pyspark\n","import random\n","sc = pyspark.SparkContext(appName=\"Pi\")\n","num_samples = 100000000\n","def inside(p):     \n","  x, y = random.random(), random.random()\n","  return x*x + y*y < 1\n","count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n","pi = 4 * count / num_samples\n","print(pi)\n","sc.stop()"],"cell_type":"code"},{"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------------------------+------------------------------------------+------+\n","|sentence                           |words                                     |tokens|\n","+-----------------------------------+------------------------------------------+------+\n","|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n","|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n","|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n","+-----------------------------------+------------------------------------------+------+\n","\n","+-----------------------------------+------------------------------------------+------+\n","|sentence                           |words                                     |tokens|\n","+-----------------------------------+------------------------------------------+------+\n","|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n","|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n","|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n","+-----------------------------------+------------------------------------------+------+\n","\n"]}],"metadata":{"collapsed":false},"execution_count":5,"source":["import findspark\n","findspark.init()\n","\n","from pyspark import SparkContext\n","from pyspark import SparkConf\n","from pyspark.sql import SparkSession\n","\n","from pyspark.ml.feature import Tokenizer, RegexTokenizer\n","from pyspark.sql.functions import col, udf\n","from pyspark.sql.types import IntegerType\n","\n","spark = SparkSession.builder.appName(\"test\").getOrCreate()\n","\n","sentenceDataFrame = spark.createDataFrame([\n","    (0, \"Hi I heard about Spark\"),\n","    (1, \"I wish Java could use case classes\"),\n","    (2, \"Logistic,regression,models,are,neat\")\n","], [\"id\", \"sentence\"])\n","\n","tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n","\n","regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n","# alternatively, pattern=\"\\\\w+\", gaps(False)\n","\n","countTokens = udf(lambda words: len(words), IntegerType())\n","\n","\n","tokenized = tokenizer.transform(sentenceDataFrame)\n","tokenized.select(\"sentence\", \"words\")\\\n","    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n","\n","regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n","regexTokenized.select(\"sentence\", \"words\") \\\n","    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"],"cell_type":"code"},{"outputs":[{"output_type":"stream","name":"stdout","text":["(4, spark i j k) --> prob=[0.334518607597,0.665481392403], prediction=1.000000\n","(5, l m n) --> prob=[0.668102477307,0.331897522693], prediction=0.000000\n","(6, spark hadoop spark) --> prob=[0.111525601523,0.888474398477], prediction=1.000000\n","(7, apache hadoop) --> prob=[0.668102477307,0.331897522693], prediction=0.000000\n"]}],"metadata":{"collapsed":false},"execution_count":16,"source":["from pyspark.ml import Pipeline\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.feature import HashingTF, Tokenizer\n","\n","# Prepare training documents from a list of (id, text, label) tuples.\n","training = spark.createDataFrame([\n","    (0, \"a b c d e spark\", 1.0),\n","    (1, \"b d\", 0.0),\n","    (2, \"spark f g h\", 1.0),\n","    (3, \"hadoop mapreduce\", 0.0)\n","], [\"id\", \"text\", \"label\"])\n","\n","# Configure an ML pipeline, which consists of one stages: tokenizer, hashingTF, and lr.\n","tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n","hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n","lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n","pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n","\n","# Fit the pipeline to training documents.\n","model = pipeline.fit(training)\n","\n","# Prepare test documents, which are unlabeled (id, text) tuples.11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n","test = spark.createDataFrame([\n","    (4, \"spark i j k\"),\n","    (5, \"l m n\"),\n","    (6, \"spark hadoop spark\"),\n","    (7, \"apache hadoop\")\n","], [\"id\", \"text\"])\n","\n","# Make predictions on test documents and print columns of interest.\n","prediction = model.transform(test)\n","selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n","for row in selected.collect():\n","    rid, text, prob, prediction = row\n","    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"],"cell_type":"code"},{"outputs":[{"output_type":"stream","name":"stdout","text":["+------+--------------------+--------------------+\n","|   _c0|                 _c1|                 _c2|\n","+------+--------------------+--------------------+\n","|100001|            Bullet01|Versatile connect...|\n","|100001|            Bullet02|Stronger than ang...|\n","|100001|            Bullet03|Help ensure joint...|\n","|100001|            Bullet04|Dimensions: 3 in....|\n","|100001|            Bullet05|Made from 12-Gaug...|\n","|100001|            Bullet06|Galvanized for ex...|\n","|100001|            Bullet07|Install with 10d ...|\n","|100001|               Gauge|                  12|\n","|100001|            Material|    Galvanized Steel|\n","|100001|      MFG Brand Name|  Simpson Strong-Tie|\n","|100001|    Number of Pieces|                   1|\n","|100001| Product Depth (in.)|                 1.5|\n","|100001|Product Height (in.)|                   3|\n","|100001|Product Weight (lb.)|                0.26|\n","|100001| Product Width (in.)|                   3|\n","|100002|  Application Method|  Brush,Roller,Spray|\n","|100002|Assembled Depth (...|             6.63 in|\n","|100002|Assembled Height ...|             7.76 in|\n","|100002|Assembled Width (...|             6.63 in|\n","|100002|            Bullet01|Revives wood and ...|\n","+------+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"metadata":{"collapsed":false},"execution_count":14,"source":["import findspark\n","findspark.init()\n","\n","from pyspark import SparkContext\n","from pyspark import SparkConf\n","from pyspark.sql import SparkSession\n","\n","df = spark.read.load(\"/Users/yiwang/Documents/YiWang/Ebiz/Task 15/attributes.csv\", format=\"csv\")\n","\n","df.show()"],"cell_type":"code"},{"outputs":[{"output_type":"stream","name":"stdout","text":["+------+--------------------+--------------------+-----+\n","|   pid|               title|                term|score|\n","+------+--------------------+--------------------+-----+\n","|100001|Simpson Strong-Ti...|       angle bracket|  3.0|\n","|100001|Simpson Strong-Ti...|           l bracket|  2.5|\n","|100002|BEHR Premium Text...|           deck over|  3.0|\n","|100005|Delta Vero 1-Hand...|    rain shower head| 2.33|\n","|100005|Delta Vero 1-Hand...|  shower only faucet| 2.67|\n","|100006|Whirlpool 1.9 cu....|      convection otr|  3.0|\n","|100006|Whirlpool 1.9 cu....|microwave over stove| 2.67|\n","|100006|Whirlpool 1.9 cu....|          microwaves|  3.0|\n","|100007|Lithonia Lighting...|     emergency light| 2.67|\n","|100009|House of Fara 3/4...|             mdf 3/4|  3.0|\n","+------+--------------------+--------------------+-----+\n","only showing top 10 rows\n","\n","+------+--------------------+\n","|   pid|         description|\n","+------+--------------------+\n","|100002|BEHR Premium Text...|\n","|100003|Classic architect...|\n","|100004|The Grape Solar 2...|\n","|100005|Update your bathr...|\n","|100006|Achieving delicio...|\n","|100007|The Quantum Adjus...|\n","|100008|The Teks #10 x 1-...|\n","|100009|Get the House of ...|\n","|100010|Valley View Indus...|\n","|100011|\"Recycler 22 in. ...|\n","+------+--------------------+\n","only showing top 10 rows\n","\n","+------+--------------------+--------------------+-----+--------------------+------------+--------------+-----------------+\n","|   pid|               title|                term|score|         description|       brand|      material|            color|\n","+------+--------------------+--------------------+-----+--------------------+------------+--------------+-----------------+\n","|100170|Dyna-Glo Pro 125,...|     kerosene heater|  1.0|Dyna-Glo Pro port...|Dyna-Glo Pro|         Steel|Oranges / Peaches|\n","|100170|Dyna-Glo Pro 125,...|      lp gas heaters| 2.67|Dyna-Glo Pro port...|Dyna-Glo Pro|         Steel|Oranges / Peaches|\n","|100170|Dyna-Glo Pro 125,...|   portable air tank| 1.67|Dyna-Glo Pro port...|Dyna-Glo Pro|         Steel|Oranges / Peaches|\n","|100170|Dyna-Glo Pro 125,...| thin propane heater| 2.33|Dyna-Glo Pro port...|Dyna-Glo Pro|         Steel|Oranges / Peaches|\n","|100274|Milwaukee M12 12-...|        milwakee M12|  3.0|Milwaukee REDLITH...|   Milwaukee|          null|              Red|\n","|100274|Milwaukee M12 12-...|       milwaukee m12| 1.67|Milwaukee REDLITH...|   Milwaukee|          null|              Red|\n","|100274|Milwaukee M12 12-...|photoelectric/ion...| 1.67|Milwaukee REDLITH...|   Milwaukee|          null|              Red|\n","|100446|Glacier Bay 2-pie...|  glaciar bay toiled| 2.67|Choose a half flu...| Glacier Bay|Vitreous China|            Beige|\n","|100446|Glacier Bay 2-pie...|glacier bay high ...| 1.67|Choose a half flu...| Glacier Bay|Vitreous China|            Beige|\n","|100446|Glacier Bay 2-pie...|  toilet glacier bay|  3.0|Choose a half flu...| Glacier Bay|Vitreous China|            Beige|\n","+------+--------------------+--------------------+-----+--------------------+------------+--------------+-----------------+\n","only showing top 10 rows\n","\n"]}],"metadata":{"collapsed":false},"execution_count":83,"source":["import findspark\n","findspark.init()\n","\n","from pyspark import SparkContext\n","from pyspark import SparkConf\n","from pyspark.sql import SparkSession\n","from pyspark.sql import SQLContext\n","from pyspark.sql.types import StructType, StructField\n","from pyspark.sql.types import DoubleType,StringType,IntegerType\n","\n","sc = spark.sparkContext\n","sql_sc = SQLContext(sc)\n","\n","trainSchema = StructType([\n","    StructField(\"id\", IntegerType()),\n","    StructField(\"pid\", IntegerType()),\n","    StructField(\"title\", StringType()),\n","    StructField(\"term\", StringType()),\n","    StructField(\"score\", DoubleType())\n","])\n","\n","titleSchema = StructType([\n","    StructField(\"pid\", IntegerType()),\n","    StructField(\"title\", StringType())\n","])\n","\n","descriptionSchema = StructType([\n","    StructField(\"pid\", IntegerType()),\n","    StructField(\"description\", StringType())\n","])\n","\n","attrSchema = StructType([\n","    StructField(\"pid\", IntegerType()),\n","    StructField(\"name\", StringType()),\n","    StructField(\"value\", StringType()),\n","])\n","\n","title = sql_sc.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").schema(trainSchema).load(\"/Users/yiwang/Documents/YiWang/Ebiz/Task 15/train.csv\")\n","train = sql_sc.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").schema(titleSchema).load(\"/Users/yiwang/Documents/YiWang/Ebiz/Task 15/RawTrain.csv\")\n","test=sql_sc.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\")    .schema(dataSchema).load(\"/Users/yiwang/Documents/YiWang/Ebiz/Task 15/test.csv\")\n","attr = sql_sc.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").schema(attrSchema).load(\"/Users/yiwang/Documents/YiWang/Ebiz/Task 15/attributes.csv\")\n","\n","description = sql_sc.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").schema(descriptionSchema).load(\"/Users/yiwang/Documents/YiWang/Ebiz/Task 15/product_descriptions.csv\")\n","title= title.drop(title.id)\n","title.show(10)\n","description.show(10)\n","\n","\n","attr.createOrReplaceTempView(\"attr\")\n","#get brand, color and material\n","brand = sql_sc.sql(\"SELECT pid,value as brand from attr where name = 'MFG Brand Name'\")\n","material = sql_sc.sql(\"SELECT pid,value as material from attr where name = 'Material'\")\n","color = sql_sc.sql(\"SELECT pid,value as color from attr where name = 'Color Family'\")\n","\n","#result=train.union(test)\n","title=title.join(description, title.pid == description.pid, \"left\").drop(description.pid)\n","title=title.join(brand, title.pid == brand.pid, \"left\").drop(brand.pid)\n","title=title.join(material, title.pid== material.pid,\"left\").drop(material.pid)\n","title=title.join(color, title.pid == color.pid,\"left\").drop(color.pid)\n","\n","title.show(10)\n"],"cell_type":"code"},{"outputs":[{"output_type":"stream","name":"stdout","text":["+------+--------------------+--------------------+-----+--------------------+------------+--------------+-----------------+\n","|   pid|               title|                term|score|         description|       brand|      material|            color|\n","+------+--------------------+--------------------+-----+--------------------+------------+--------------+-----------------+\n","|100170|Dyna-Glo Pro 125,...|     kerosene heater|  1.0|Dyna-Glo Pro port...|Dyna-Glo Pro|         Steel|Oranges / Peaches|\n","|100170|Dyna-Glo Pro 125,...|      lp gas heaters| 2.67|Dyna-Glo Pro port...|Dyna-Glo Pro|         Steel|Oranges / Peaches|\n","|100170|Dyna-Glo Pro 125,...|   portable air tank| 1.67|Dyna-Glo Pro port...|Dyna-Glo Pro|         Steel|Oranges / Peaches|\n","|100170|Dyna-Glo Pro 125,...| thin propane heater| 2.33|Dyna-Glo Pro port...|Dyna-Glo Pro|         Steel|Oranges / Peaches|\n","|100274|Milwaukee M12 12-...|        milwakee M12|  3.0|Milwaukee REDLITH...|   Milwaukee|         empty|              Red|\n","|100274|Milwaukee M12 12-...|       milwaukee m12| 1.67|Milwaukee REDLITH...|   Milwaukee|         empty|              Red|\n","|100274|Milwaukee M12 12-...|photoelectric/ion...| 1.67|Milwaukee REDLITH...|   Milwaukee|         empty|              Red|\n","|100446|Glacier Bay 2-pie...|  glaciar bay toiled| 2.67|Choose a half flu...| Glacier Bay|Vitreous China|            Beige|\n","|100446|Glacier Bay 2-pie...|glacier bay high ...| 1.67|Choose a half flu...| Glacier Bay|Vitreous China|            Beige|\n","|100446|Glacier Bay 2-pie...|  toilet glacier bay|  3.0|Choose a half flu...| Glacier Bay|Vitreous China|            Beige|\n","+------+--------------------+--------------------+-----+--------------------+------------+--------------+-----------------+\n","only showing top 10 rows\n","\n"]}],"metadata":{"collapsed":false},"execution_count":84,"source":["from pyspark.sql.functions import col, when\n","title=title.withColumn(\n","    \"color\", when(col(\"color\").isNull(), \"empty\").otherwise(col(\"color\")))\n","title=title.withColumn(\n","    \"brand\", when(col(\"brand\").isNull(), \"empty\").otherwise(col(\"brand\")))\n","title=title.withColumn(\n","    \"material\", when(col(\"material\").isNull(), \"empty\").otherwise(col(\"material\")))\n","title=title.withColumn(\n","    \"description\", when(col(\"description\").isNull(), \"empty\").otherwise(col(\"description\")))\n","\n","\n","title.show(10)"],"cell_type":"code"},{"outputs":[{"output_type":"stream","name":"stdout","text":["+------+--------------------+-----+--------------------+\n","|   pid|                term|score|       joined_column|\n","+------+--------------------+-----+--------------------+\n","|100170|     kerosene heater|  1.0|Dyna-Glo Pro port...|\n","|100170|      lp gas heaters| 2.67|Dyna-Glo Pro port...|\n","|100170|   portable air tank| 1.67|Dyna-Glo Pro port...|\n","|100170| thin propane heater| 2.33|Dyna-Glo Pro port...|\n","|100274|        milwakee M12|  3.0|Milwaukee REDLITH...|\n","|100274|       milwaukee m12| 1.67|Milwaukee REDLITH...|\n","|100274|photoelectric/ion...| 1.67|Milwaukee REDLITH...|\n","|100446|  glaciar bay toiled| 2.67|Choose a half flu...|\n","|100446|glacier bay high ...| 1.67|Choose a half flu...|\n","|100446|  toilet glacier bay|  3.0|Choose a half flu...|\n","+------+--------------------+-----+--------------------+\n","only showing top 10 rows\n","\n"]}],"metadata":{"collapsed":false},"execution_count":85,"source":["from pyspark.sql import functions as sf\n","title = title.withColumn('joined_column', \n","                    sf.concat( sf.col('description'),sf.lit('_'), sf.col('title'),sf.lit('_'), sf.col('brand'), sf.lit('_'), sf.col('material'),sf.lit('_'), sf.col('color')))\n","title = title.drop(title.title).drop(title.description).drop(title.brand).drop(title.material).drop(title.color)\n","title.show(10)"],"cell_type":"code"},{"outputs":[{"output_type":"stream","name":"stdout","text":["+------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|   pid|                term|score|       joined_column|          term_words|            term_idf|        joined_words|          joined_idf|\n","+------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|100170|     kerosene heater|  1.0|Dyna-Glo Pro port...|  [kerosene, heater]|(10,[0,8],[1.1296...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|\n","|100170|      lp gas heaters| 2.67|Dyna-Glo Pro port...|  [lp, gas, heaters]|(10,[2,5,9],[1.26...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|\n","|100170|   portable air tank| 1.67|Dyna-Glo Pro port...|[portable, air, t...|(10,[3,4,7],[1.25...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|\n","|100170| thin propane heater| 2.33|Dyna-Glo Pro port...|[thin, propane, h...|(10,[0,2,7],[1.12...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|\n","|100274|        milwakee M12|  3.0|Milwaukee REDLITH...|     [milwakee, m12]|(10,[4,6],[1.2325...|[milwaukee, redli...|(10,[0,1,2,3,4,5,...|\n","|100274|       milwaukee m12| 1.67|Milwaukee REDLITH...|    [milwaukee, m12]|(10,[0,6],[1.1296...|[milwaukee, redli...|(10,[0,1,2,3,4,5,...|\n","|100274|photoelectric/ion...| 1.67|Milwaukee REDLITH...|[photoelectric/io...|(10,[0,2,4],[1.12...|[milwaukee, redli...|(10,[0,1,2,3,4,5,...|\n","|100446|  glaciar bay toiled| 2.67|Choose a half flu...|[glaciar, bay, to...|(10,[1,9],[2.7061...|[choose, a, half,...|(10,[0,1,2,3,4,5,...|\n","|100446|glacier bay high ...| 1.67|Choose a half flu...|[glacier, bay, hi...|(10,[1,6,7,9],[1....|[choose, a, half,...|(10,[0,1,2,3,4,5,...|\n","|100446|  toilet glacier bay|  3.0|Choose a half flu...|[toilet, glacier,...|(10,[1,9],[1.3530...|[choose, a, half,...|(10,[0,1,2,3,4,5,...|\n","|100446|  toilets in biscuit|  3.0|Choose a half flu...|[toilets, in, bis...|(10,[5,6,9],[1.40...|[choose, a, half,...|(10,[0,1,2,3,4,5,...|\n","|100800|coating sealer fo...|  2.0|Seal-Krete Epoxy ...|[coating, sealer,...|(10,[2,3,5,6],[1....|[seal-krete, epox...|(10,[0,1,2,3,4,5,...|\n","|100986|            slatwall|  1.0|Keep your garage ...|          [slatwall]|(10,[1],[1.353078...|[keep, your, gara...|(10,[0,1,2,3,4,5,...|\n","|101055|12 x 12 concrete ...|  2.0|The Pavestone 0.5...|[12, x, 12, concr...|(10,[0,6],[4.5184...|[the, pavestone, ...|(10,[0,1,2,3,4,5,...|\n","|101055|      12 x 12 pavers| 2.67|The Pavestone 0.5...| [12, x, 12, pavers]|(10,[0,6],[3.3888...|[the, pavestone, ...|(10,[0,1,2,3,4,5,...|\n","|101055|        12x12 pavers| 1.67|The Pavestone 0.5...|     [12x12, pavers]|(10,[6,9],[1.3081...|[the, pavestone, ...|(10,[0,1,2,3,4,5,...|\n","|101055|   paver base gravel| 2.67|The Pavestone 0.5...|[paver, base, gra...|(10,[3,4,9],[1.25...|[the, pavestone, ...|(10,[0,1,2,3,4,5,...|\n","|101055|        paver stones| 2.33|The Pavestone 0.5...|     [paver, stones]|(10,[9],[2.362761...|[the, pavestone, ...|(10,[0,1,2,3,4,5,...|\n","|101055|        paving brick| 1.67|The Pavestone 0.5...|     [paving, brick]|(10,[5,7],[1.4026...|[the, pavestone, ...|(10,[0,1,2,3,4,5,...|\n","|101094|1 lite french doo...| 2.33|Clear Pine is a s...|[1, lite, french,...|(10,[2,3,4,8],[1....|[clear, pine, is,...|(10,[0,1,2,3,4,5,...|\n","+------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"metadata":{"collapsed":false},"execution_count":93,"source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n","#tokenize terms\n","tokenizer = Tokenizer(inputCol=\"term\", outputCol=\"term_words\")\n","temp = tokenizer.transform(title)\n","\n","hashingTF = HashingTF(inputCol=\"term_words\", outputCol=\"rawFeatures\", numFeatures=10)\n","temp = hashingTF.transform(temp)\n","\n","idf = IDF(inputCol = \"rawFeatures\", outputCol=\"term_idf\")\n","idfModel = idf.fit(temp)\n","temp = idfModel.transform(temp)\n","temp=temp.drop(\"rawFeatures\")\n","\n","#tokenize joined_column\n","tokenizer = Tokenizer(inputCol=\"joined_column\", outputCol=\"joined_words\")\n","temp = tokenizer.transform(temp)\n","\n","hashingTF = HashingTF(inputCol=\"joined_words\", outputCol=\"rawFeatures\", numFeatures=10)\n","temp = hashingTF.transform(temp)\n","\n","idf = IDF(inputCol = \"rawFeatures\", outputCol=\"joined_idf\")\n","idfModel = idf.fit(temp)\n","temp = idfModel.transform(temp)\n","temp=temp.drop(\"rawFeatures\")\n","\n","temp.show(5)\n"],"cell_type":"code"},{"outputs":[{"output_type":"stream","name":"stdout","text":["+------+-------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|   pid|               term|score|       joined_column|          term_words|            term_idf|        joined_words|          joined_idf|\n","+------+-------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|100170|    kerosene heater|  1.0|Dyna-Glo Pro port...|  [kerosene, heater]|(10,[0,8],[1.1296...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|\n","|100170|     lp gas heaters| 2.67|Dyna-Glo Pro port...|  [lp, gas, heaters]|(10,[2,5,9],[1.26...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|\n","|100170|  portable air tank| 1.67|Dyna-Glo Pro port...|[portable, air, t...|(10,[3,4,7],[1.25...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|\n","|100170|thin propane heater| 2.33|Dyna-Glo Pro port...|[thin, propane, h...|(10,[0,2,7],[1.12...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|\n","|100274|       milwakee M12|  3.0|Milwaukee REDLITH...|     [milwakee, m12]|(10,[4,6],[1.2325...|[milwaukee, redli...|(10,[0,1,2,3,4,5,...|\n","+------+-------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n","only showing top 5 rows\n","\n"]}],"metadata":{"collapsed":false},"execution_count":94,"source":["result = temp\n","result.show(5)"],"cell_type":"code"},{"outputs":[{"output_type":"stream","name":"stdout","text":["+------+-------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n","|   pid|               term|score|       joined_column|          term_words|            term_idf|        joined_words|          joined_idf|match|\n","+------+-------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n","|100170|    kerosene heater|  1.0|Dyna-Glo Pro port...|  [kerosene, heater]|(10,[0,8],[1.1296...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|    0|\n","|100170|     lp gas heaters| 2.67|Dyna-Glo Pro port...|  [lp, gas, heaters]|(10,[2,5,9],[1.26...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|    0|\n","|100170|  portable air tank| 1.67|Dyna-Glo Pro port...|[portable, air, t...|(10,[3,4,7],[1.25...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|    0|\n","|100170|thin propane heater| 2.33|Dyna-Glo Pro port...|[thin, propane, h...|(10,[0,2,7],[1.12...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|    0|\n","|100274|       milwakee M12|  3.0|Milwaukee REDLITH...|     [milwakee, m12]|(10,[4,6],[1.2325...|[milwaukee, redli...|(10,[0,1,2,3,4,5,...|    0|\n","+------+-------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n","only showing top 5 rows\n","\n"]}],"metadata":{"collapsed":false},"execution_count":96,"source":["from pyspark.sql.types import *\n","from pyspark.sql import SQLContext\n","from pyspark.sql.functions import udf\n","\n","#match words and then proceed to jaccard_similarity_score\n","def countMatchedWords(joined, term):\n","    l1=len(joined)\n","    l2=len(term)\n","    match = 0\n","    for i in range(l1):\n","        for j in range(l2):\n","            if joined[i] == term[j]:\n","                match+=2\n","            elif joined[i] in term[j]:\n","                match+=1\n","            elif term[j] in joined[i]:\n","                match+=1\n","        return match\n","matchUDF=udf(countMatchedWords, IntegerType())\n","\n","result=result.withColumn(\"match\", matchUDF(\"joined_words\", \"term_words\"))\n","result.show(5)"],"cell_type":"code"},{"outputs":[{"output_type":"stream","name":"stderr","text":["Exception ignored in: <object repr() failed>\n","Traceback (most recent call last):\n","  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/ml/wrapper.py\", line 76, in __del__\n","    SparkContext._active_spark_context._gateway.detach(self._java_obj)\n","AttributeError: 'Tokenizer' object has no attribute '_java_obj'\n","Exception ignored in: <object repr() failed>\n","Traceback (most recent call last):\n","  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/ml/wrapper.py\", line 76, in __del__\n","    SparkContext._active_spark_context._gateway.detach(self._java_obj)\n","AttributeError: 'HashingTF' object has no attribute '_java_obj'\n","Exception ignored in: <object repr() failed>\n","Traceback (most recent call last):\n","  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/ml/wrapper.py\", line 76, in __del__\n","    SparkContext._active_spark_context._gateway.detach(self._java_obj)\n","AttributeError: 'Tokenizer' object has no attribute '_java_obj'\n","Exception ignored in: <object repr() failed>\n","Traceback (most recent call last):\n","  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/ml/wrapper.py\", line 76, in __del__\n","    SparkContext._active_spark_context._gateway.detach(self._java_obj)\n","AttributeError: 'HashingTF' object has no attribute '_java_obj'\n","Exception ignored in: <object repr() failed>\n","Traceback (most recent call last):\n","  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/ml/wrapper.py\", line 76, in __del__\n","    SparkContext._active_spark_context._gateway.detach(self._java_obj)\n","AttributeError: 'HashingTF' object has no attribute '_java_obj'\n"]},{"ename":"Py4JJavaError","output_type":"error","evalue":"An error occurred while calling o3736.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 315.0 failed 1 times, most recent failure: Lost task 0.0 in stage 315.0 (TID 2007, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 70, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-97-fdb29be1d2a5>\", line 3, in jaccardSimilarity\n  File \"/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py\", line 426, in jaccard_similarity_score\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n  File \"/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py\", line 72, in _check_targets\n    check_consistent_length(y_true, y_pred)\n  File \"/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 181, in check_consistent_length\n    \" samples: %r\" % [int(l) for l in lengths])\nValueError: Found input variables with inconsistent numbers of samples: [15, 615]\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2386)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2385)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2392)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2128)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2818)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2342)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n\tat sun.reflect.GeneratedMethodAccessor164.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 70, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-97-fdb29be1d2a5>\", line 3, in jaccardSimilarity\n  File \"/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py\", line 426, in jaccard_similarity_score\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n  File \"/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py\", line 72, in _check_targets\n    check_consistent_length(y_true, y_pred)\n  File \"/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 181, in check_consistent_length\n    \" samples: %r\" % [int(l) for l in lengths])\nValueError: Found input variables with inconsistent numbers of samples: [15, 615]\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-97-fdb29be1d2a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"term_joined_jaccard\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaccardUDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"term\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"joined_column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3736.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 315.0 failed 1 times, most recent failure: Lost task 0.0 in stage 315.0 (TID 2007, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 70, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-97-fdb29be1d2a5>\", line 3, in jaccardSimilarity\n  File \"/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py\", line 426, in jaccard_similarity_score\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n  File \"/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py\", line 72, in _check_targets\n    check_consistent_length(y_true, y_pred)\n  File \"/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 181, in check_consistent_length\n    \" samples: %r\" % [int(l) for l in lengths])\nValueError: Found input variables with inconsistent numbers of samples: [15, 615]\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2386)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2385)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2392)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2128)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2818)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2342)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n\tat sun.reflect.GeneratedMethodAccessor164.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 70, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-97-fdb29be1d2a5>\", line 3, in jaccardSimilarity\n  File \"/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py\", line 426, in jaccard_similarity_score\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n  File \"/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py\", line 72, in _check_targets\n    check_consistent_length(y_true, y_pred)\n  File \"/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 181, in check_consistent_length\n    \" samples: %r\" % [int(l) for l in lengths])\nValueError: Found input variables with inconsistent numbers of samples: [15, 615]\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"]}],"metadata":{"collapsed":false},"execution_count":97,"source":["from sklearn.metrics import jaccard_similarity_score\n","def jaccardSimilarity(term, joined):\n","    result=float(jaccard_similarity_score(term, joined))\n","    return result\n","jaccardUDF=udf(jaccardSimilarity, DoubleType())\n","\n","result = result.withColumn(\"term_joined_jaccard\", jaccardUDF(\"term\", \"joined_column\"))\n","result.show(5)"],"cell_type":"code"},{"outputs":[],"metadata":{"collapsed":true},"execution_count":null,"source":[],"cell_type":"code"}],"nbformat":4}