{  "cells": [   {    "cell_type": "code",    "execution_count": 55,    "metadata": {     "collapsed": false    },    "outputs": [     {      "name": "stdout",      "output_type": "stream",      "text": [       "DataFrame[id: int, pid: int, title: string, term: string, score: double]\n",       "DataFrame[pid: int, description: string]\n"      ]     }    ],    "source": [     "import findspark\n",     "findspark.init()\n",     "\n",     "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",     "from pyspark import SparkContext\n",     "from pyspark import SparkConf\n",     "from pyspark.sql import SparkSession\n",     "from pyspark.sql import SQLContext\n",     "from pyspark.sql.types import StructType, StructField\n",     "from pyspark.sql.types import DoubleType,StringType,IntegerType\n",     "\n",     "sc = spark.sparkContext\n",     "sql_sc = SQLContext(sc)\n",     "\n",     "trainSchema = StructType([\n",     "    StructField(\"id\", IntegerType()),\n",     "    StructField(\"pid\", IntegerType()),\n",     "    StructField(\"title\", StringType()),\n",     "    StructField(\"term\", StringType()),\n",     "    StructField(\"score\", DoubleType())\n",     "])\n",     "\n",     "titleSchema = StructType([\n",     "    StructField(\"id\", IntegerType()),\n",     "    StructField(\"pid\", IntegerType()),\n",     "    StructField(\"title\", StringType()),\n",     "    StructField(\"term\", StringType())\n",     "])\n",     "\n",     "testSchema = StructType([\n",     "    StructField(\"pid\", IntegerType()),\n",     "    StructField(\"title\", StringType())\n",     "])\n",     "\n",     "descriptionSchema = StructType([\n",     "    StructField(\"pid\", IntegerType()),\n",     "    StructField(\"description\", StringType())\n",     "])\n",     "\n",     "attrSchema = StructType([\n",     "    StructField(\"pid\", IntegerType()),\n",     "    StructField(\"name\", StringType()),\n",     "    StructField(\"value\", StringType()),\n",     "])\n",     "\n",     "title = sql_sc.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").schema(trainSchema).load(\"/Users/yiwang/Documents/YiWang/Ebiz/Task15/train.csv\")\n",     "train = sql_sc.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").schema(titleSchema).load(\"/Users/yiwang/Documents/YiWang/Ebiz/Task15/RawTrain.csv\")\n",     "attr = sql_sc.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").schema(attrSchema).load(\"/Users/yiwang/Documents/YiWang/Ebiz/Task15/attributes.csv\")\n",     "test = sql_sc.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").schema(testSchema).load(\"/Users/yiwang/Documents/YiWang/Ebiz/Task15/test-panda.csv\")\n",     "description = sql_sc.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").schema(descriptionSchema).load(\"/Users/yiwang/Documents/YiWang/Ebiz/Task15/product_descriptions.csv\")\n",     "#title= title.drop(title.id)\n",     "print(title)\n",     "print(description)\n",     "\n",     "\n",     "attr.createOrReplaceTempView(\"attr\")\n",     "#get brand, color and material\n",     "brand = sql_sc.sql(\"SELECT pid,value as brand from attr where name = 'MFG Brand Name'\")\n",     "material = sql_sc.sql(\"SELECT pid,value as material from attr where name = 'Material'\")\n",     "color = sql_sc.sql(\"SELECT pid,value as color from attr where name = 'Color Family'\")\n",     "\n",     "#result=train.union(test)\n",     "title=title.join(description, title.pid == description.pid, \"left\").drop(description.pid)\n",     "title=title.join(brand, title.pid == brand.pid, \"left\").drop(brand.pid)\n",     "title=title.join(material, title.pid== material.pid,\"left\").drop(material.pid)\n",     "title=title.join(color, title.pid == color.pid,\"left\").drop(color.pid)\n",     "\n",     "\n"    ]   },   {    "cell_type": "code",    "execution_count": 56,    "metadata": {     "collapsed": false    },    "outputs": [     {      "name": "stdout",      "output_type": "stream",      "text": [       "DataFrame[id: int, pid: int, title: string, term: string, score: double, description: string, brand: string, material: string, color: string]\n"      ]     }    ],    "source": [     "from pyspark.sql.functions import col, when\n",     "title=title.withColumn(\n",     "    \"color\", when(col(\"color\").isNull(), \"empty\").otherwise(col(\"color\")))\n",     "title=title.withColumn(\n",     "    \"brand\", when(col(\"brand\").isNull(), \"empty\").otherwise(col(\"brand\")))\n",     "title=title.withColumn(\n",     "    \"material\", when(col(\"material\").isNull(), \"empty\").otherwise(col(\"material\")))\n",     "title=title.withColumn(\n",     "    \"description\", when(col(\"description\").isNull(), \"empty\").otherwise(col(\"description\")))\n",     "\n",     "\n",     "print(title)"    ]   },   {    "cell_type": "code",    "execution_count": 57,    "metadata": {     "collapsed": false    },    "outputs": [     {      "name": "stdout",      "output_type": "stream",      "text": [       "DataFrame[id: int, pid: int, term: string, score: double, joined_column: string]\n"      ]     }    ],    "source": [     "from pyspark.sql import functions as sf\n",     "title = title.withColumn('joined_column', \n",     "                    sf.concat( sf.col('description'),sf.lit('_'), sf.col('title'),sf.lit('_'), sf.col('brand'), sf.lit('_'), sf.col('material'),sf.lit('_'), sf.col('color')))\n",     "title = title.drop(title.title).drop(title.description).drop(title.brand).drop(title.material).drop(title.color)\n",     "print(title)"    ]   },   {    "cell_type": "code",    "execution_count": 58,    "metadata": {     "collapsed": false    },    "outputs": [     {      "name": "stdout",      "output_type": "stream",      "text": [       "+----+------+-------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",       "|  id|   pid|               term|score|       joined_column|          term_words|            term_idf|        joined_words|          joined_idf|\n",       "+----+------+-------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",       "|1019|100170|    kerosene heater|  1.0|Dyna-Glo Pro port...|  [kerosene, heater]|(10,[0,8],[1.1296...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|\n",       "|1020|100170|     lp gas heaters| 2.67|Dyna-Glo Pro port...|  [lp, gas, heaters]|(10,[2,5,9],[1.26...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|\n",       "|1022|100170|  portable air tank| 1.67|Dyna-Glo Pro port...|[portable, air, t...|(10,[3,4,7],[1.25...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|\n",       "|1026|100170|thin propane heater| 2.33|Dyna-Glo Pro port...|[thin, propane, h...|(10,[0,2,7],[1.12...|[dyna-glo, pro, p...|(10,[0,1,2,3,4,5,...|\n",       "|1576|100274|       milwakee M12|  3.0|Milwaukee REDLITH...|     [milwakee, m12]|(10,[4,6],[1.2325...|[milwaukee, redli...|(10,[0,1,2,3,4,5,...|\n",       "+----+------+-------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",       "only showing top 5 rows\n",       "\n"      ]     }    ],    "source": [     "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",     "#tokenize terms\n",     "tokenizer = Tokenizer(inputCol=\"term\", outputCol=\"term_words\")\n",     "temp = tokenizer.transform(title)\n",     "\n",     "hashingTF = HashingTF(inputCol=\"term_words\", outputCol=\"rawFeatures\", numFeatures=10)\n",     "temp = hashingTF.transform(temp)\n",     "\n",     "idf = IDF(inputCol = \"rawFeatures\", outputCol=\"term_idf\")\n",     "idfModel = idf.fit(temp)\n",     "temp = idfModel.transform(temp)\n",     "temp=temp.drop(\"rawFeatures\")\n",     "\n",     "#tokenize joined_column\n",     "tokenizer = Tokenizer(inputCol=\"joined_column\", outputCol=\"joined_words\")\n",     "temp = tokenizer.transform(temp)\n",     "\n",     "hashingTF = HashingTF(inputCol=\"joined_words\", outputCol=\"rawFeatures\", numFeatures=10)\n",     "temp = hashingTF.transform(temp)\n",     "\n",     "idf = IDF(inputCol = \"rawFeatures\", outputCol=\"joined_idf\")\n",     "idfModel = idf.fit(temp)\n",     "temp = idfModel.transform(temp)\n",     "temp=temp.drop(\"rawFeatures\")\n",     "\n",     "temp.show(5)\n"    ]   },   {    "cell_type": "code",    "execution_count": 59,    "metadata": {     "collapsed": false    },    "outputs": [],    "source": [     "result = temp\n",     "#result.show(5)"    ]   },   {